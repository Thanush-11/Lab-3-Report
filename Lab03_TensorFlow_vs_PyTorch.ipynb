{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ebc05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ebc05e",
        "outputId": "24c0a299-6c6e-44b4-bddc-08794750b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3042 - accuracy: 0.9144\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1461 - accuracy: 0.9573\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1060 - accuracy: 0.9684\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0836 - accuracy: 0.9746\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0689 - accuracy: 0.9795\n",
            "TF Training time: 19.97 seconds\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.9729\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.08611072599887848, 0.9728999733924866]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Fill name of loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Thanush\\AppData\\Local\\Temp\\tmp2r253mqb\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Thanush\\AppData\\Local\\Temp\\tmp2r253mqb\\assets\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c2db9a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torch) (4.7.1)\n",
            "Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n",
            "   ---------------------------------------- 162.6/162.6 MB 2.0 MB/s eta 0:00:00\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0450f82b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "  Downloading torchvision-0.14.1-cp37-cp37m-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision) (4.7.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==1.13.1 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision) (1.13.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchvision) (9.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->torchvision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->torchvision) (2024.8.30)\n",
            "Downloading torchvision-0.14.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
            "   ---------------------------------------- 1.1/1.1 MB 3.1 MB/s eta 0:00:00\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:14<00:00, 664451.41it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 186664.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1612778.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "PyTorch Training time: 39.67 seconds\n",
            "Test accuracy: 0.9681\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)    # Fill correct input and output size\n",
        "        self.fc2 = nn.Linear(64, 10)    # Fill correct input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))    # Fill correct layer\n",
        "        return self.fc2(x)    # Fill correct layer\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "WuMKMhHc8aLF",
      "metadata": {
        "id": "WuMKMhHc8aLF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.14.1-cp37-cp37m-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (1.21.6)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-4.24.4-cp37-cp37m-win_amd64.whl.metadata (540 bytes)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from onnx) (4.7.1)\n",
            "Downloading onnx-1.14.1-cp37-cp37m-win_amd64.whl (13.3 MB)\n",
            "   ---------------------------------------- 13.3/13.3 MB 3.1 MB/s eta 0:00:00\n",
            "Downloading protobuf-4.24.4-cp37-cp37m-win_amd64.whl (430 kB)\n",
            "   ---------------------------------------- 430.0/430.0 kB 2.7 MB/s eta 0:00:00\n",
            "Installing collected packages: protobuf, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "Successfully installed onnx-1.14.1 protobuf-4.24.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'c:\\users\\thanush\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\google\\~rotobuf'.\n",
            "  You can safely remove it manually.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n",
            "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sv4P-dSS_GQB",
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "KH-sDlHq_Gdw",
      "metadata": {
        "id": "KH-sDlHq_Gdw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4268, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.4096, Accuracy: 0.7197\n",
            "Step 200, Loss: 0.4075, Accuracy: 0.7946\n",
            "Step 300, Loss: 0.3447, Accuracy: 0.8286\n",
            "Step 400, Loss: 0.1733, Accuracy: 0.8480\n",
            "Step 500, Loss: 0.5225, Accuracy: 0.8609\n",
            "Step 600, Loss: 0.1019, Accuracy: 0.8698\n",
            "Step 700, Loss: 0.3136, Accuracy: 0.8773\n",
            "Step 800, Loss: 0.6158, Accuracy: 0.8830\n",
            "Step 900, Loss: 0.2619, Accuracy: 0.8886\n",
            "Step 1000, Loss: 0.2120, Accuracy: 0.8935\n",
            "Step 1100, Loss: 0.5958, Accuracy: 0.8969\n",
            "Step 1200, Loss: 0.0543, Accuracy: 0.8998\n",
            "Step 1300, Loss: 0.4926, Accuracy: 0.9025\n",
            "Step 1400, Loss: 0.2422, Accuracy: 0.9048\n",
            "Step 1500, Loss: 0.2736, Accuracy: 0.9079\n",
            "Step 1600, Loss: 0.0895, Accuracy: 0.9105\n",
            "Step 1700, Loss: 0.0291, Accuracy: 0.9129\n",
            "Step 1800, Loss: 0.0474, Accuracy: 0.9150\n",
            "Training Accuracy for epoch 1: 0.9166\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0905, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1602, Accuracy: 0.9511\n",
            "Step 200, Loss: 0.0784, Accuracy: 0.9523\n",
            "Step 300, Loss: 0.0223, Accuracy: 0.9523\n",
            "Step 400, Loss: 0.1170, Accuracy: 0.9536\n",
            "Step 500, Loss: 0.0743, Accuracy: 0.9550\n",
            "Step 600, Loss: 0.0604, Accuracy: 0.9564\n",
            "Step 700, Loss: 0.0283, Accuracy: 0.9571\n",
            "Step 800, Loss: 0.0537, Accuracy: 0.9574\n",
            "Step 900, Loss: 0.0424, Accuracy: 0.9568\n",
            "Step 1000, Loss: 0.0850, Accuracy: 0.9569\n",
            "Step 1100, Loss: 0.0539, Accuracy: 0.9568\n",
            "Step 1200, Loss: 0.1432, Accuracy: 0.9571\n",
            "Step 1300, Loss: 0.2400, Accuracy: 0.9572\n",
            "Step 1400, Loss: 0.1862, Accuracy: 0.9574\n",
            "Step 1500, Loss: 0.1015, Accuracy: 0.9578\n",
            "Step 1600, Loss: 0.0199, Accuracy: 0.9583\n",
            "Step 1700, Loss: 0.1169, Accuracy: 0.9588\n",
            "Step 1800, Loss: 0.1032, Accuracy: 0.9593\n",
            "Training Accuracy for epoch 2: 0.9595\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0948, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1643, Accuracy: 0.9691\n",
            "Step 200, Loss: 0.0397, Accuracy: 0.9664\n",
            "Step 300, Loss: 0.0351, Accuracy: 0.9676\n",
            "Step 400, Loss: 0.0710, Accuracy: 0.9690\n",
            "Step 500, Loss: 0.1505, Accuracy: 0.9689\n",
            "Step 600, Loss: 0.2323, Accuracy: 0.9693\n",
            "Step 700, Loss: 0.0398, Accuracy: 0.9692\n",
            "Step 800, Loss: 0.0273, Accuracy: 0.9691\n",
            "Step 900, Loss: 0.1596, Accuracy: 0.9685\n",
            "Step 1000, Loss: 0.1166, Accuracy: 0.9687\n",
            "Step 1100, Loss: 0.1088, Accuracy: 0.9691\n",
            "Step 1200, Loss: 0.0681, Accuracy: 0.9690\n",
            "Step 1300, Loss: 0.0725, Accuracy: 0.9695\n",
            "Step 1400, Loss: 0.0060, Accuracy: 0.9696\n",
            "Step 1500, Loss: 0.0640, Accuracy: 0.9695\n",
            "Step 1600, Loss: 0.0944, Accuracy: 0.9697\n",
            "Step 1700, Loss: 0.0352, Accuracy: 0.9698\n",
            "Step 1800, Loss: 0.0859, Accuracy: 0.9698\n",
            "Training Accuracy for epoch 3: 0.9699\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0632, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0579, Accuracy: 0.9734\n",
            "Step 200, Loss: 0.0730, Accuracy: 0.9743\n",
            "Step 300, Loss: 0.0667, Accuracy: 0.9749\n",
            "Step 400, Loss: 0.0393, Accuracy: 0.9763\n",
            "Step 500, Loss: 0.2036, Accuracy: 0.9764\n",
            "Step 600, Loss: 0.2587, Accuracy: 0.9757\n",
            "Step 700, Loss: 0.0187, Accuracy: 0.9756\n",
            "Step 800, Loss: 0.0083, Accuracy: 0.9756\n",
            "Step 900, Loss: 0.0383, Accuracy: 0.9756\n",
            "Step 1000, Loss: 0.0462, Accuracy: 0.9760\n",
            "Step 1100, Loss: 0.1897, Accuracy: 0.9761\n",
            "Step 1200, Loss: 0.2044, Accuracy: 0.9756\n",
            "Step 1300, Loss: 0.0062, Accuracy: 0.9755\n",
            "Step 1400, Loss: 0.0967, Accuracy: 0.9753\n",
            "Step 1500, Loss: 0.0429, Accuracy: 0.9754\n",
            "Step 1600, Loss: 0.0798, Accuracy: 0.9755\n",
            "Step 1700, Loss: 0.0804, Accuracy: 0.9756\n",
            "Step 1800, Loss: 0.0822, Accuracy: 0.9758\n",
            "Training Accuracy for epoch 4: 0.9759\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0150, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0209, Accuracy: 0.9802\n",
            "Step 200, Loss: 0.0478, Accuracy: 0.9823\n",
            "Step 300, Loss: 0.0626, Accuracy: 0.9801\n",
            "Step 400, Loss: 0.0715, Accuracy: 0.9814\n",
            "Step 500, Loss: 0.1843, Accuracy: 0.9818\n",
            "Step 600, Loss: 0.1257, Accuracy: 0.9809\n",
            "Step 700, Loss: 0.0468, Accuracy: 0.9810\n",
            "Step 800, Loss: 0.0219, Accuracy: 0.9804\n",
            "Step 900, Loss: 0.0317, Accuracy: 0.9803\n",
            "Step 1000, Loss: 0.0281, Accuracy: 0.9803\n",
            "Step 1100, Loss: 0.0280, Accuracy: 0.9804\n",
            "Step 1200, Loss: 0.0289, Accuracy: 0.9799\n",
            "Step 1300, Loss: 0.1313, Accuracy: 0.9799\n",
            "Step 1400, Loss: 0.0487, Accuracy: 0.9797\n",
            "Step 1500, Loss: 0.2822, Accuracy: 0.9796\n",
            "Step 1600, Loss: 0.0165, Accuracy: 0.9795\n",
            "Step 1700, Loss: 0.0036, Accuracy: 0.9794\n",
            "Step 1800, Loss: 0.0116, Accuracy: 0.9797\n",
            "Training Accuracy for epoch 5: 0.9799\n",
            "\n",
            "TF Training time: 173.43 seconds\n",
            "Test Accuracy: 0.9722\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Fill same batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E4Nlg4lb_qdW",
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Jmu_hciK_qle",
      "metadata": {
        "id": "Jmu_hciK_qle"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4117, Accuracy: 0.0625\n",
            "Step 100, Loss: 0.5608, Accuracy: 0.7277\n",
            "Step 200, Loss: 0.5111, Accuracy: 0.8015\n",
            "Step 300, Loss: 0.4082, Accuracy: 0.8325\n",
            "Step 400, Loss: 0.2351, Accuracy: 0.8535\n",
            "Step 500, Loss: 0.1655, Accuracy: 0.8646\n",
            "Step 600, Loss: 0.1575, Accuracy: 0.8739\n",
            "Step 700, Loss: 0.1227, Accuracy: 0.8810\n",
            "Step 800, Loss: 0.4570, Accuracy: 0.8858\n",
            "Step 900, Loss: 0.1589, Accuracy: 0.8899\n",
            "Step 1000, Loss: 0.1822, Accuracy: 0.8939\n",
            "Step 1100, Loss: 0.2359, Accuracy: 0.8967\n",
            "Step 1200, Loss: 0.2501, Accuracy: 0.8993\n",
            "Step 1300, Loss: 0.4514, Accuracy: 0.9020\n",
            "Step 1400, Loss: 0.2580, Accuracy: 0.9042\n",
            "Step 1500, Loss: 0.2796, Accuracy: 0.9066\n",
            "Step 1600, Loss: 0.0921, Accuracy: 0.9090\n",
            "Step 1700, Loss: 0.0983, Accuracy: 0.9112\n",
            "Step 1800, Loss: 0.1283, Accuracy: 0.9134\n",
            "Training Accuracy for epoch 1: 0.9147\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.2011, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.2911, Accuracy: 0.9465\n",
            "Step 200, Loss: 0.2193, Accuracy: 0.9487\n",
            "Step 300, Loss: 0.1942, Accuracy: 0.9510\n",
            "Step 400, Loss: 0.1977, Accuracy: 0.9518\n",
            "Step 500, Loss: 0.2065, Accuracy: 0.9522\n",
            "Step 600, Loss: 0.0897, Accuracy: 0.9530\n",
            "Step 700, Loss: 0.0625, Accuracy: 0.9533\n",
            "Step 800, Loss: 0.2572, Accuracy: 0.9530\n",
            "Step 900, Loss: 0.2033, Accuracy: 0.9521\n",
            "Step 1000, Loss: 0.0552, Accuracy: 0.9522\n",
            "Step 1100, Loss: 0.1169, Accuracy: 0.9528\n",
            "Step 1200, Loss: 0.0436, Accuracy: 0.9531\n",
            "Step 1300, Loss: 0.1510, Accuracy: 0.9535\n",
            "Step 1400, Loss: 0.1410, Accuracy: 0.9539\n",
            "Step 1500, Loss: 0.1214, Accuracy: 0.9540\n",
            "Step 1600, Loss: 0.1219, Accuracy: 0.9544\n",
            "Step 1700, Loss: 0.4172, Accuracy: 0.9547\n",
            "Step 1800, Loss: 0.1324, Accuracy: 0.9551\n",
            "Training Accuracy for epoch 2: 0.9554\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0930, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0727, Accuracy: 0.9619\n",
            "Step 200, Loss: 0.0636, Accuracy: 0.9641\n",
            "Step 300, Loss: 0.0574, Accuracy: 0.9619\n",
            "Step 400, Loss: 0.1272, Accuracy: 0.9625\n",
            "Step 500, Loss: 0.0784, Accuracy: 0.9639\n",
            "Step 600, Loss: 0.1167, Accuracy: 0.9641\n",
            "Step 700, Loss: 0.0170, Accuracy: 0.9650\n",
            "Step 800, Loss: 0.0385, Accuracy: 0.9648\n",
            "Step 900, Loss: 0.0491, Accuracy: 0.9648\n",
            "Step 1000, Loss: 0.0664, Accuracy: 0.9656\n",
            "Step 1100, Loss: 0.0836, Accuracy: 0.9657\n",
            "Step 1200, Loss: 0.0729, Accuracy: 0.9657\n",
            "Step 1300, Loss: 0.0888, Accuracy: 0.9659\n",
            "Step 1400, Loss: 0.2102, Accuracy: 0.9659\n",
            "Step 1500, Loss: 0.1458, Accuracy: 0.9659\n",
            "Step 1600, Loss: 0.0674, Accuracy: 0.9660\n",
            "Step 1700, Loss: 0.0878, Accuracy: 0.9663\n",
            "Step 1800, Loss: 0.0494, Accuracy: 0.9665\n",
            "Training Accuracy for epoch 3: 0.9667\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1337, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0187, Accuracy: 0.9703\n",
            "Step 200, Loss: 0.0252, Accuracy: 0.9722\n",
            "Step 300, Loss: 0.0876, Accuracy: 0.9732\n",
            "Step 400, Loss: 0.1634, Accuracy: 0.9732\n",
            "Step 500, Loss: 0.0355, Accuracy: 0.9733\n",
            "Step 600, Loss: 0.0145, Accuracy: 0.9732\n",
            "Step 700, Loss: 0.0401, Accuracy: 0.9722\n",
            "Step 800, Loss: 0.2531, Accuracy: 0.9724\n",
            "Step 900, Loss: 0.2377, Accuracy: 0.9722\n",
            "Step 1000, Loss: 0.1726, Accuracy: 0.9729\n",
            "Step 1100, Loss: 0.0476, Accuracy: 0.9731\n",
            "Step 1200, Loss: 0.0133, Accuracy: 0.9732\n",
            "Step 1300, Loss: 0.0045, Accuracy: 0.9735\n",
            "Step 1400, Loss: 0.2441, Accuracy: 0.9729\n",
            "Step 1500, Loss: 0.0561, Accuracy: 0.9729\n",
            "Step 1600, Loss: 0.0409, Accuracy: 0.9730\n",
            "Step 1700, Loss: 0.0107, Accuracy: 0.9731\n",
            "Step 1800, Loss: 0.2781, Accuracy: 0.9731\n",
            "Training Accuracy for epoch 4: 0.9731\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0433, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0561, Accuracy: 0.9802\n",
            "Step 200, Loss: 0.3133, Accuracy: 0.9795\n",
            "Step 300, Loss: 0.0358, Accuracy: 0.9790\n",
            "Step 400, Loss: 0.0127, Accuracy: 0.9791\n",
            "Step 500, Loss: 0.0540, Accuracy: 0.9794\n",
            "Step 600, Loss: 0.0563, Accuracy: 0.9788\n",
            "Step 700, Loss: 0.1210, Accuracy: 0.9781\n",
            "Step 800, Loss: 0.3385, Accuracy: 0.9778\n",
            "Step 900, Loss: 0.0228, Accuracy: 0.9777\n",
            "Step 1000, Loss: 0.0364, Accuracy: 0.9777\n",
            "Step 1100, Loss: 0.0051, Accuracy: 0.9772\n",
            "Step 1200, Loss: 0.0447, Accuracy: 0.9771\n",
            "Step 1300, Loss: 0.0341, Accuracy: 0.9769\n",
            "Step 1400, Loss: 0.0232, Accuracy: 0.9769\n",
            "Step 1500, Loss: 0.0816, Accuracy: 0.9766\n",
            "Step 1600, Loss: 0.2548, Accuracy: 0.9769\n",
            "Step 1700, Loss: 0.0027, Accuracy: 0.9772\n",
            "Step 1800, Loss: 0.0196, Accuracy: 0.9774\n",
            "Training Accuracy for epoch 5: 0.9772\n",
            "\n",
            "TF Training time: 35.46 seconds\n",
            "Test Accuracy: 0.9695\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
